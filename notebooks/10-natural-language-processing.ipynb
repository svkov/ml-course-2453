{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02c65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a211e7",
   "metadata": {},
   "source": [
    "# Обработка естественного языка\n",
    "\n",
    "Где используется:\n",
    "\n",
    "- Анализ соц.сетей\n",
    "- Голосовые помощники, чат-боты\n",
    "- Веб-поиск\n",
    "- Машинный перевод\n",
    "- Анализ тематики текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea050d",
   "metadata": {},
   "source": [
    "## Работаем со строками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7d5b0",
   "metadata": {},
   "source": [
    "По строкам можно индексироваться как и по спискам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4fa899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'арозА упал ан алапу азоР'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Роза упала на лапу Азора\"\n",
    "sentence[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d2dd2",
   "metadata": {},
   "source": [
    "Строки можно разбивать на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645413ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Роза', 'упала', 'на', 'лапу', 'Азора']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566d147",
   "metadata": {},
   "source": [
    "Массивы строк можно соединять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ff5748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Роза/упала/на/лапу/Азора'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/\".join(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1f212",
   "metadata": {},
   "source": [
    "Можно менять регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa80a467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('роза упала на лапу азора', 'РОЗА УПАЛА НА ЛАПУ АЗОРА')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.lower(), sentence.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f8d22",
   "metadata": {},
   "source": [
    "Проверять на числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c1b720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.isnumeric(), sentence.isalpha() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da5899",
   "metadata": {},
   "source": [
    "Можно делать замены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6369d4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'не роза упала на лапу Азора'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.replace('Роза', 'не роза')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97e69c",
   "metadata": {},
   "source": [
    "Есть специальные символы, которые записываются через `\\` и позволяют сделать перенос строки, табуляцию и многое другое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc1722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "первая строка\n",
      "вторая строка\n",
      "до\tпосле\n",
      "Пишем кавычки: ', \"\n",
      "Пишем обратный слэш: \\\n"
     ]
    }
   ],
   "source": [
    "print('первая строка\\nвторая строка') # перенос строки\n",
    "print('до\\tпосле') # табуляция\n",
    "print('Пишем кавычки: \\', \\\"')\n",
    "print('Пишем обратный слэш: \\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595795c",
   "metadata": {},
   "source": [
    "Также существуют регулярные выражения, с помощью которых можно сделать практически что угодно с любым набором символов.\n",
    "\n",
    "Ниже пример проверки на валидность адреса электронной почты. \n",
    "\n",
    "В реальном приложении лучше не использовать такую проверку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "542956b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Email\n",
      "Invalid Email\n",
      "Invalid Email\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "\n",
    "# Don't use it in prod\n",
    "def check(email):\n",
    "    if(re.fullmatch(regex, email)):\n",
    "        print(\"Valid Email\") \n",
    "    else:\n",
    "        print(\"Invalid Email\")\n",
    "        \n",
    "check('valid_email@mail.ru')\n",
    "check('invalid email')\n",
    "check('invalid_email@mail.21')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fec94",
   "metadata": {},
   "source": [
    "## Задача\n",
    "\n",
    "Предобработать текст следующим способом:\n",
    "\n",
    "- разбить текст на слова\n",
    "- привести все к нижнему регистру\n",
    "- убрать из текста все точки, запятые и скобки\n",
    "\n",
    "Найти в тексте самое частовстречаемое слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4590e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Разобьем этот текст на слова, приведем к нижнему регистру. Затем уберем пунктуацию (точки, запятые и скобки). А потом найдем слово, которое встречается чаще всего. текст текст текст.\"\n",
    "\n",
    "# Ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a383c",
   "metadata": {},
   "source": [
    "## Bag of word\n",
    "\n",
    "Чтобы применять к тексту методы машинного обучения, его необходимо векторизовать (превратить в массив чисел). Это можно сделать разными способами. Самый простой способ - bag of word.\n",
    "\n",
    "Разбиваем весь текст на токены (слова). Затем формируем матрицу, в которой в строках храним предложения, а в колонках слова. За каждое вхождение слова в предложение добавляем 1 в соответствующую ячейку. Не забываем убрать знаки препинания, табуляцию, пробелы и знаки переноса строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03241c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b14dc545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first document.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This is the second second document.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And the third one.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     and  document  first  is  one  second  \\\n",
       "This is the first document.            0         1      1   1    0       0   \n",
       "This is the second second document.    0         1      0   1    0       2   \n",
       "And the third one.                     1         0      0   0    1       0   \n",
       "Is this the first document?            0         1      1   1    0       0   \n",
       "\n",
       "                                     the  third  this  \n",
       "This is the first document.            1      0     1  \n",
       "This is the second second document.    1      0     1  \n",
       "And the third one.                     1      1     0  \n",
       "Is this the first document?            1      0     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "bow = CountVectorizer()\n",
    "bow_result = bow.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(bow_result, columns=bow.get_feature_names(), index=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508cd70d",
   "metadata": {},
   "source": [
    "С русским текстом это будет работать не очень хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5a56cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>вот</th>\n",
       "      <th>всех</th>\n",
       "      <th>второй</th>\n",
       "      <th>документ</th>\n",
       "      <th>документов</th>\n",
       "      <th>документы</th>\n",
       "      <th>кончились</th>\n",
       "      <th>первый</th>\n",
       "      <th>среди</th>\n",
       "      <th>третий</th>\n",
       "      <th>это</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Это первый документ.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Это второй документ среди всех документов.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>А вот и третий.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Вот и кончились документы.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            вот  всех  второй  документ  \\\n",
       "Это первый документ.                          0     0       0         1   \n",
       "Это второй документ среди всех документов.    0     1       1         1   \n",
       "А вот и третий.                               1     0       0         0   \n",
       "Вот и кончились документы.                    1     0       0         0   \n",
       "\n",
       "                                            документов  документы  кончились  \\\n",
       "Это первый документ.                                 0          0          0   \n",
       "Это второй документ среди всех документов.           1          0          0   \n",
       "А вот и третий.                                      0          0          0   \n",
       "Вот и кончились документы.                           0          1          1   \n",
       "\n",
       "                                            первый  среди  третий  это  \n",
       "Это первый документ.                             1      0       0    1  \n",
       "Это второй документ среди всех документов.       0      1       0    1  \n",
       "А вот и третий.                                  0      0       1    0  \n",
       "Вот и кончились документы.                       0      0       0    0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Это первый документ.',\n",
    "    'Это второй документ среди всех документов.',\n",
    "    'А вот и третий.',\n",
    "    'Вот и кончились документы.',\n",
    "]\n",
    "\n",
    "bow = CountVectorizer()\n",
    "bow_result = bow.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(bow_result, columns=bow.get_feature_names(), index=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd4ede",
   "metadata": {},
   "source": [
    "\"документ\", \"документов\", \"документы\" - это все об одном, но колонки получились разные. Что делать с этим?\n",
    "\n",
    "Применить стемминг!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4b6e9",
   "metadata": {},
   "source": [
    "## Стемминг\n",
    "\n",
    "Стемминг - процесс извлечения основы слова, потому что зачастую форма слова не влияет на смысл.\n",
    "\n",
    "В классе будем использовать довольно простой стеммер Портера. На практике лучше использовать библиотеку `nltk` для русского текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6231b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Взято отсюда: https://gist.github.com/Kein1945/9111512\n",
    "import re\n",
    "\n",
    "class Porter:\n",
    "\tPERFECTIVEGROUND =  re.compile(u\"((ив|ивши|ившись|ыв|ывши|ывшись)|((?<=[ая])(в|вши|вшись)))$\")\n",
    "\tREFLEXIVE = re.compile(u\"(с[яь])$\")\n",
    "\tADJECTIVE = re.compile(u\"(ее|ие|ые|ое|ими|ыми|ей|ий|ый|ой|ем|им|ым|ом|его|ого|ему|ому|их|ых|ую|юю|ая|яя|ою|ею)$\")\n",
    "\tPARTICIPLE = re.compile(u\"((ивш|ывш|ующ)|((?<=[ая])(ем|нн|вш|ющ|щ)))$\")\n",
    "\tVERB = re.compile(u\"((ила|ыла|ена|ейте|уйте|ите|или|ыли|ей|уй|ил|ыл|им|ым|ен|ило|ыло|ено|ят|ует|уют|ит|ыт|ены|ить|ыть|ишь|ую|ю)|((?<=[ая])(ла|на|ете|йте|ли|й|л|ем|н|ло|но|ет|ют|ны|ть|ешь|нно)))$\")\n",
    "\tNOUN = re.compile(u\"(а|ев|ов|ие|ье|е|иями|ями|ами|еи|ии|и|ией|ей|ой|ий|й|иям|ям|ием|ем|ам|ом|о|у|ах|иях|ях|ы|ь|ию|ью|ю|ия|ья|я)$\")\n",
    "\tRVRE = re.compile(u\"^(.*?[аеиоуыэюя])(.*)$\")\n",
    "\tDERIVATIONAL = re.compile(u\".*[^аеиоуыэюя]+[аеиоуыэюя].*ость?$\")\n",
    "\tDER = re.compile(u\"ость?$\")\n",
    "\tSUPERLATIVE = re.compile(u\"(ейше|ейш)$\")\n",
    "\tI = re.compile(u\"и$\")\n",
    "\tP = re.compile(u\"ь$\")\n",
    "\tNN = re.compile(u\"нн$\")\n",
    "\n",
    "\tdef stem(word):\n",
    "\t\tword = word.lower()\n",
    "\t\tword = word.replace(u'ё', u'е')\n",
    "\t\tm = re.match(Porter.RVRE, word)\n",
    "\t\tif m and m.groups():\n",
    "\t\t\tpre = m.group(1)\n",
    "\t\t\trv = m.group(2)\n",
    "\t\t\ttemp = Porter.PERFECTIVEGROUND.sub('', rv, 1)\n",
    "\t\t\tif temp == rv:\n",
    "\t\t\t\trv = Porter.REFLEXIVE.sub('', rv, 1)\n",
    "\t\t\t\ttemp = Porter.ADJECTIVE.sub('', rv, 1)\n",
    "\t\t\t\tif temp != rv:\n",
    "\t\t\t\t\trv = temp\n",
    "\t\t\t\t\trv = Porter.PARTICIPLE.sub('', rv, 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp = Porter.VERB.sub('', rv, 1)\n",
    "\t\t\t\t\tif temp == rv:\n",
    "\t\t\t\t\t\trv = Porter.NOUN.sub('', rv, 1)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\trv = temp\n",
    "\t\t\telse:\n",
    "\t\t\t\trv = temp\n",
    "\t\t\t\n",
    "\t\t\trv = Porter.I.sub('', rv, 1)\n",
    "\n",
    "\t\t\tif re.match(Porter.DERIVATIONAL, rv):\n",
    "\t\t\t\trv = Porter.DER.sub('', rv, 1)\n",
    "\n",
    "\t\t\ttemp = Porter.P.sub('', rv, 1)\n",
    "\t\t\tif temp == rv:\n",
    "\t\t\t\trv = Porter.SUPERLATIVE.sub('', rv, 1)\n",
    "\t\t\t\trv = Porter.NN.sub(u'н', rv, 1)\n",
    "\t\t\telse:\n",
    "\t\t\t\trv = temp\n",
    "\t\t\tword = pre+rv\n",
    "\t\treturn word\n",
    "\tstem=staticmethod(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe292464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('документ', 'документ', 'документ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Porter.stem('документа'), Porter.stem('документы'), Porter.stem('документов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cedcb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    return Porter.stem(word.replace('.', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b14b0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['эт перв документ',\n",
       " 'эт втор документ сред всех документ',\n",
       " 'а вот и трет',\n",
       " 'вот и конч документ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_corpus = [' '.join(map(preprocess_word, text.split())) for text in corpus]\n",
    "stemmed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8515cea8",
   "metadata": {},
   "source": [
    "Вынесем в отдельную функцию, она нам дальше понадобится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fb70cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    return [' '.join(map(preprocess_word, text.split())) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cbcdc",
   "metadata": {},
   "source": [
    "Запустим Bag of word на новом корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a320f174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>вот</th>\n",
       "      <th>всех</th>\n",
       "      <th>втор</th>\n",
       "      <th>документ</th>\n",
       "      <th>конч</th>\n",
       "      <th>перв</th>\n",
       "      <th>сред</th>\n",
       "      <th>трет</th>\n",
       "      <th>эт</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>эт перв документ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>эт втор документ сред всех документ</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>а вот и трет</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>вот и конч документ</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     вот  всех  втор  документ  конч  перв  \\\n",
       "эт перв документ                       0     0     0         1     0     1   \n",
       "эт втор документ сред всех документ    0     1     1         2     0     0   \n",
       "а вот и трет                           1     0     0         0     0     0   \n",
       "вот и конч документ                    1     0     0         1     1     0   \n",
       "\n",
       "                                     сред  трет  эт  \n",
       "эт перв документ                        0     0   1  \n",
       "эт втор документ сред всех документ     1     0   1  \n",
       "а вот и трет                            0     1   0  \n",
       "вот и конч документ                     0     0   0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer()\n",
    "bow_result = bow.fit_transform(stemmed_corpus).toarray()\n",
    "pd.DataFrame(bow_result, columns=bow.get_feature_names(), index=stemmed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6712e",
   "metadata": {},
   "source": [
    "Если хотим подставить исходные предложения, передаем другое значение в index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc559d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>вот</th>\n",
       "      <th>всех</th>\n",
       "      <th>втор</th>\n",
       "      <th>документ</th>\n",
       "      <th>конч</th>\n",
       "      <th>перв</th>\n",
       "      <th>сред</th>\n",
       "      <th>трет</th>\n",
       "      <th>эт</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Это первый документ.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Это второй документ среди всех документов.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>А вот и третий.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Вот и кончились документы.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            вот  всех  втор  документ  конч  \\\n",
       "Это первый документ.                          0     0     0         1     0   \n",
       "Это второй документ среди всех документов.    0     1     1         2     0   \n",
       "А вот и третий.                               1     0     0         0     0   \n",
       "Вот и кончились документы.                    1     0     0         1     1   \n",
       "\n",
       "                                            перв  сред  трет  эт  \n",
       "Это первый документ.                           1     0     0   1  \n",
       "Это второй документ среди всех документов.     0     1     0   1  \n",
       "А вот и третий.                                0     0     1   0  \n",
       "Вот и кончились документы.                     0     0     0   0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow_result, columns=bow.get_feature_names(), index=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03cb72",
   "metadata": {},
   "source": [
    "## Попробуем взять документ побольше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c6dd8",
   "metadata": {},
   "source": [
    "Сначала распакуем архив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8d1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25678705",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_path = '../data/news.txt.gz'\n",
    "file_path = '../data/news.txt'\n",
    "\n",
    "with gzip.open(archive_path, 'rb') as arch:\n",
    "    content = arch.read()\n",
    "    \n",
    "with open(file_path, 'wb') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6ee2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>style</td>\n",
       "      <td>Rolex наградит победителей регаты</td>\n",
       "      <td>Парусная гонка Giraglia Rolex Cup пройдет в Ср...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sport</td>\n",
       "      <td>Матс Сундин стал советником тренера сборной Шв...</td>\n",
       "      <td>Шведский хоккеист Матс Сундин назначен советни...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>media</td>\n",
       "      <td>Брендом года по версии EFFIE впервые стал город</td>\n",
       "      <td>Гран-при конкурса \"Брэнд года/EFFIE\" получил г...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>economics</td>\n",
       "      <td>Цена нефти WTI снизилась после публикации данн...</td>\n",
       "      <td>Цена американской нефти WTI на лондонской бирж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economics</td>\n",
       "      <td>Сбербанк распродаст другим банкирам миллиардны...</td>\n",
       "      <td>Сбербанк выставил на продажу долги по 21,4 тыс...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              title  \\\n",
       "0      style                  Rolex наградит победителей регаты   \n",
       "1      sport  Матс Сундин стал советником тренера сборной Шв...   \n",
       "2      media    Брендом года по версии EFFIE впервые стал город   \n",
       "3  economics  Цена нефти WTI снизилась после публикации данн...   \n",
       "4  economics  Сбербанк распродаст другим банкирам миллиардны...   \n",
       "\n",
       "                                                text  \n",
       "0  Парусная гонка Giraglia Rolex Cup пройдет в Ср...  \n",
       "1  Шведский хоккеист Матс Сундин назначен советни...  \n",
       "2  Гран-при конкурса \"Брэнд года/EFFIE\" получил г...  \n",
       "3  Цена американской нефти WTI на лондонской бирж...  \n",
       "4  Сбербанк выставил на продажу долги по 21,4 тыс...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/news.txt', sep='\\t', header=None)\n",
    "df.columns = ['label', 'title', 'text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "044ed1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.label.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa99071",
   "metadata": {},
   "source": [
    "## Задача\n",
    "\n",
    "Применить стемминг к тексту и добавить колонку `text_stemmed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5af790df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b174a",
   "metadata": {},
   "source": [
    "## Задача\n",
    "\n",
    "Спрогнозируем тему новости.\n",
    "\n",
    "Выборку разделить на train и test, применить логистическую регрессию к датасету, чтобы классифицировать тему.\n",
    "\n",
    "Для оценки качества использовать метрику accuracy. Вывести confusion matrix.\n",
    "\n",
    "**Важно:** на вход логистической регрессии необходимо подавать матрицу из чисел, а не слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d86f63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bba90806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = pd.Categorical(df['label'])\n",
    "df['label_code'] = df.label.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d26370ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648508d",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "BOW - очень простой алгоритм, но даже он дает неплохой результат. Попробуем его улучшить. \n",
    "\n",
    "`TF-IDF` - формула, состоящая из двух частей **Term frequency - Inverce Document Frequency**\n",
    "\n",
    "`TF` - какой процент указанного слова по отношению ко всем словам в документе\n",
    "\n",
    "`IDF` - количество документов делим на количество документов, в которые входит данное слово\n",
    "\n",
    "`TF-IDF` = `TF` * `IDF`\n",
    "\n",
    "Класс `TfidfVectorizer` имеет такие же методы, как и `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "23c175c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d688eec",
   "metadata": {},
   "source": [
    "## Задача \n",
    "\n",
    "Применить TF-IDF к тому же датасету, сравнить качество с BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403afc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4eddc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Пайплайны\n",
    "\n",
    "Можно использовать сложные преобразования и легко сравнивать качество моделей, а также подбирать гиперпараметры, используя пайплайны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4b1efeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2b32d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", LogisticRegression()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff10a8",
   "metadata": {},
   "source": [
    "Для примера обучимся на всей выборке и попробуем что-то предсказать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0f54cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koval\\anaconda3\\envs\\data-science-class\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(df['text'], df['label_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3c7da9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Парусная гонка Giraglia Rolex Cup пройдет в Ср...\n",
       "1    Шведский хоккеист Матс Сундин назначен советни...\n",
       "2    Гран-при конкурса \"Брэнд года/EFFIE\" получил г...\n",
       "3    Цена американской нефти WTI на лондонской бирж...\n",
       "4    Сбербанк выставил на продажу долги по 21,4 тыс...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5048ea4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8\n",
       "1    7\n",
       "2    5\n",
       "3    2\n",
       "4    2\n",
       "Name: label_code, dtype: int8"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_code'].iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "787c5ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 7, 5, 2, 2], dtype=int8)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(df['text'].iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1100121",
   "metadata": {},
   "source": [
    "Подробнее можно посмотреть [тут](https://www.kaggle.com/sermakarevich/sklearn-pipelines-tutorial)\n",
    "\n",
    "Мы вернемся к пайплайнам через несколько занятий и рассмотрим инструменты, которые дают большую гибкость, чем sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ca768",
   "metadata": {},
   "source": [
    "## Задача\n",
    "\n",
    "Попробовать случайный лес для классификации документов. Использовать пайплайны для решения.\n",
    "\n",
    "Выборку необходимо как и раньше делить на треин и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc45bb",
   "metadata": {},
   "source": [
    "## Лемматизаторы\n",
    "\n",
    "Лемматизация - поиск начальной формы слова.\n",
    "\n",
    "Используя лемматизаторы можно более точно различать слова, но работают они медленнее, чем стемммеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ef4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f88205b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'случайность'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "morph_res = morph.parse('случайности')\n",
    "morph_res[0].normal_form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6fb31",
   "metadata": {},
   "source": [
    "## Задача\n",
    "\n",
    "Применить лемматизацию перед TF-IDF. Решить задачу классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9cdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f17ae74",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Все предыдущие методы позволяли посмотреть частоту вхождения слова, но при этом не учитывали смысл слова и похожесть слов.\n",
    "\n",
    "Метод word2vec пытается понять смысл слов.\n",
    "\n",
    "![](../images/word2vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee61b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(df):\n",
    "    \"\"\"\n",
    "    Разбиваем на слова, убираем пунктуацию и применяем стемминг\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for i, text in enumerate(df['text']):\n",
    "        if i % 500 == 0:\n",
    "            print(i) # Отмечаем прогресс\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text) # Убираем пунктуацию, разбиваем на слова\n",
    "        tokens = [Porter.stem(word) for word in words] # Применяем стемминг\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2948b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n"
     ]
    }
   ],
   "source": [
    "processed_texts = process_sentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b0b8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab8cdac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49300205, 58933440)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(processed_texts, total_examples=len(processed_texts), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d5aa98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('биатлон', 0.6496457457542419),\n",
       " ('теннис', 0.6235681772232056),\n",
       " ('футбол', 0.600563645362854),\n",
       " ('баскетбол', 0.5875694155693054),\n",
       " ('фигурн', 0.551959216594696),\n",
       " ('триатлон', 0.5348076224327087),\n",
       " ('волейбол', 0.5223391652107239),\n",
       " ('фехтован', 0.5153805017471313),\n",
       " ('чемпионк', 0.5122454762458801),\n",
       " ('катан', 0.501915693283081)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'хоккей'\n",
    "stemmed_word = Porter.stem(word)\n",
    "model.wv.most_similar(stemmed_word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9287819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('полиц', 0.8942763209342957),\n",
       " ('полицейск', 0.8309827446937561),\n",
       " ('мужчин', 0.8248519897460938),\n",
       " ('милиц', 0.7188462018966675),\n",
       " ('пенсионерк', 0.699247419834137),\n",
       " ('преступник', 0.691802442073822),\n",
       " ('охранник', 0.6899130344390869),\n",
       " ('спасател', 0.6889834403991699),\n",
       " ('грабител', 0.6472103595733643),\n",
       " ('злоумышленник', 0.631364107131958)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_v(word):\n",
    "    return model.wv[Porter.stem(word)]\n",
    "\n",
    "model.wv.similar_by_vector(get_v('полиция') + get_v('мужчина'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ae818",
   "metadata": {},
   "source": [
    "## Задача\n",
    "\n",
    "Используя word2vec классифицировать тексты\n",
    "\n",
    "Алгоритм:\n",
    "\n",
    "- разделить на треин и тест выборки\n",
    "- обучить word2vec на треин выборке\n",
    "- написать функцию для составления вектора по документу (для этого нужно посчитать среднее векторов всех слов)\n",
    "- обучить модель, оценить результаты\n",
    "\n",
    "Для начала лучше взять небольшую выборку, чтобы проще было писать код (например, 2000 в треин и 5 примеров в тест). \n",
    "\n",
    "Когда убедились, что все работает, брать весь датасет.\n",
    "\n",
    "Функции, которые могут помочь и подсказки:\n",
    "\n",
    "- `model.wv.key_to_index` - словарь токен->номер токена. Можно взять список всех слов отсюда.\n",
    "- `model.wv[word]` - получаем вектор по слову. Не забываем применить предобработку!\n",
    "- может случиться так, что word2vec не будет знать какого-то слова из обучающей выборки. Тогда этому слову присваиваем нулевой вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d501a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0398f692",
   "metadata": {},
   "source": [
    "# Домашняя работа\n",
    "\n",
    "Будем решать задачу сентимент-анализа. В архиве `data/sentiment analisys.zip` лежит несколько эксель файлов с твитами разных пользователей на разных языках (один файл - один язык).\n",
    "\n",
    "Нужно выбрать один из файлов и провести над ним работу.\n",
    "\n",
    "## Easy\n",
    "\n",
    "Оценить сбалансированность классов.\n",
    "\n",
    "Удалить пунктуацию из датасета.\n",
    "\n",
    "Преобразовать датасет в BOW или TF-IDF, поделить на треин-тест и спрогнозировать вашей любимой моделью\n",
    "\n",
    "## Normal\n",
    "\n",
    "Удалить стоп-слова из датасета, использовать стеммер Портера (лежит в `src/porter.py`). Использовать пайплайн для TF-IDF и модели машинного обучения. Подобрать параметры. Сравнить качество нескольких моделей.\n",
    "\n",
    "## Hard\n",
    "\n",
    "Использовать лемматизатор `pymorphy` вместо стемминга. Использовать word2vec вместо tf-idf. В качестве модели использовать градиентный бустинг не из sklearn. Подобрать параметры, оценить качество на кросс-валидации."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
